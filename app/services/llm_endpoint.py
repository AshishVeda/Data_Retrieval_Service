import os
import json
import requests
import logging
import os.path
from app.config import Config

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# First try to use Config class values
API_URL = Config.API_URL
HF_TOKEN = Config.HF_TOKEN

# If not available in Config, try loading from config file
if not API_URL:
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')
    try:
        if os.path.exists(config_path):
            with open(config_path) as config_file:
                config = json.load(config_file)
            API_URL = config.get("API_URL")
            logger.info(f"Loaded API_URL from config.json: {API_URL}")
    except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
        logger.error(f"Error loading API URL from config: {str(e)}")

# Log configuration status
if API_URL:
    logger.info(f"LLM API URL is configured: {API_URL}")
else:
    logger.error("API_URL is not configured. Please set it in .env file or config.json")

if HF_TOKEN:
    logger.info("HF_TOKEN is configured")
else:
    logger.warning("HF_TOKEN is not configured. Please set it in .env file")

# Headers
HEADERS = {
    "Accept": "application/json",
    "Authorization": f"Bearer {HF_TOKEN}" if HF_TOKEN else "",
    "Content-Type": "application/json"
}

def generate_prediction(prompt, max_new_tokens=512):
    """
    Sends the generated prompt to the LLM API and returns the raw response.
    
    Args:
        prompt (str): The prompt generated by llm_service.py
        max_new_tokens (int): Maximum number of new tokens to generate
        
    Returns:
        str: The raw text response from the LLM
    """
    if not API_URL:
        logger.error("API_URL is not set. Cannot make request to LLM.")
        return "Error: LLM API URL not configured. Please add API_URL to your .env file or config.json."
        
    if not HF_TOKEN:
        logger.error("HF_TOKEN is not set. Cannot make request to LLM.")
        return "Error: HF_TOKEN not configured. Please add HF_TOKEN to your .env file."
        
    try:
        # Most basic payload format for Hugging Face endpoints
        payload = {
            "inputs": prompt
        }
        
        logger.info(f"Sending prompt to LLM API: {prompt[:100]}...")
        logger.info(f"Using endpoint: {API_URL}")
        
        # Log the exact payload for debugging
        logger.info(f"Payload: {json.dumps(payload)}")
        
        response = requests.post(API_URL, headers=HEADERS, json=payload, timeout=60)
        
        # Log the response status
        logger.info(f"Response status code: {response.status_code}")
        
        # If there's an error, try to log the response content
        if response.status_code != 200:
            logger.error(f"Error response content: {response.text}")
            
        response.raise_for_status()  # raises HTTPError if response is bad
        
        # Extract the generated text from the response
        result = response.json()
        logger.info(f"Raw response type: {type(result)}")
        
        if isinstance(result, list) and result:
            # Handle different response formats from various API providers
            if isinstance(result[0], dict) and "generated_text" in result[0]:
                return result[0]["generated_text"]
            return result[0]
        elif isinstance(result, dict) and "generated_text" in result:
            return result["generated_text"]
        else:
            return str(result)
            
    except requests.exceptions.RequestException as e:
        logger.error(f"Error making request to LLM API: {str(e)}")
        return f"Error communicating with LLM API: {str(e)}"
    except (ValueError, KeyError, TypeError) as e:
        logger.error(f"Error processing LLM API response: {str(e)}")
        return f"Error processing LLM response: {str(e)}"
